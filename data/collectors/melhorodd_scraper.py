import os
import random
import time
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import requests
from bs4 import BeautifulSoup
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MelhorOddScraper:
    """Scraper for MelhorOdd.pt which aggregates betting odds from multiple sources."""
    
    def __init__(self, days_ahead: int = 2, cache_duration: int = 3600):
        """Initialize the MelhorOdd.pt scraper.
        
        Args:
            days_ahead: Number of days ahead to scrape
            cache_duration: Duration in seconds to cache results
        """
        self.days_ahead = days_ahead
        self.cache_duration = cache_duration
        self.cached_matches = []
        self.cached_time = datetime.min
        
        # Base URL for all games
        self.base_url = "https://www.melhorodd.pt/todos-os-jogos/"
        
        # User agents for rotation
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
        ]
    
    def _get_headers(self) -> Dict[str, str]:
        """Get random headers for the request.
        
        Returns:
            Dictionary with HTTP headers
        """
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-PT,pt;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://www.melhorodd.pt/',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'TE': 'Trailers',
        }
    
    def _fetch_page(self, url: str) -> Optional[str]:
        """Fetch the HTML content of a page.
        
        Args:
            url: URL to fetch
            
        Returns:
            HTML content of the page or None if the request failed
        """
        try:
            # Add a delay to avoid hitting rate limits
            time.sleep(random.uniform(1, 3))
            
            # Make the request
            response = requests.get(url, headers=self._get_headers(), timeout=30)
            
            # Check if the request was successful
            if response.status_code == 200:
                logger.info(f"Successfully fetched {url}")
                return response.text
            else:
                logger.error(f"Failed to fetch {url}, status code: {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None
    
    def _extract_date_from_match(self, match_time_element: Any) -> Optional[datetime]:
        """Extract date and time from match time element.
        
        Args:
            match_time_element: BeautifulSoup element containing the match time
            
        Returns:
            Datetime object or None if parsing failed
        """
        try:
            # Check if the element exists
            if not match_time_element:
                return None
            
            # Get the text from the element
            time_text = match_time_element.get_text(strip=True)
            
            # Common formats on melhorodd.pt
            # Format: "19/03 - 20:00"
            match = re.search(r'(\d{2}/\d{2})\s*[-â€“]\s*(\d{2}:\d{2})', time_text)
            
            if match:
                date_str, time_str = match.groups()
                
                # Extract day and month
                day, month = map(int, date_str.split('/'))
                hour, minute = map(int, time_str.split(':'))
                
                # Current year
                current_year = datetime.now().year
                
                # Create datetime object
                match_datetime = datetime(
                    year=current_year,
                    month=month,
                    day=day,
                    hour=hour,
                    minute=minute
                )
                
                # If the date is in the past, it might be for next year
                if match_datetime < datetime.now() - timedelta(hours=3):
                    match_datetime = match_datetime.replace(year=current_year + 1)
                
                return match_datetime
            
            return None
        except Exception as e:
            logger.warning(f"Error parsing match date: {e}")
            return None
    
    def _parse_odds(self, odds_elements: List[Any]) -> Dict[str, float]:
        """Parse odds values from odds elements.
        
        Args:
            odds_elements: List of BeautifulSoup elements containing odds
            
        Returns:
            Dictionary mapping odds types to values
        """
        odds = {
            "home_win": 0,
            "draw": 0,
            "away_win": 0,
            "over_2_5": 0,
            "under_2_5": 0,
            "btts_yes": 0,
            "btts_no": 0
        }
        
        try:
            # Process 1X2 odds (match winner)
            if len(odds_elements) >= 3:
                # Get text and convert to float for each odd
                for i, odd_type in enumerate(['home_win', 'draw', 'away_win']):
                    if i < len(odds_elements):
                        odd_text = odds_elements[i].get_text(strip=True).replace(',', '.')
                        try:
                            odds[odd_type] = float(odd_text)
                        except (ValueError, TypeError):
                            pass
            
            # Process Over/Under odds if available
            if len(odds_elements) >= 5:
                for i, odd_type in enumerate(['over_2_5', 'under_2_5']):
                    if i + 3 < len(odds_elements):
                        odd_text = odds_elements[i + 3].get_text(strip=True).replace(',', '.')
                        try:
                            odds[odd_type] = float(odd_text)
                        except (ValueError, TypeError):
                            pass
            
            # Process BTTS odds if available
            if len(odds_elements) >= 7:
                for i, odd_type in enumerate(['btts_yes', 'btts_no']):
                    if i + 5 < len(odds_elements):
                        odd_text = odds_elements[i + 5].get_text(strip=True).replace(',', '.')
                        try:
                            odds[odd_type] = float(odd_text)
                        except (ValueError, TypeError):
                            pass
        except Exception as e:
            logger.warning(f"Error parsing odds: {e}")
        
        return odds
    
    def scrape_matches(self) -> List[Dict[str, Any]]:
        """Scrape football matches from MelhorOdd.pt.
        
        Returns:
            List of dictionaries containing match information
        """
        # Check if we have cached matches and they're still valid
        if self.cached_matches and (datetime.now() - self.cached_time).total_seconds() < self.cache_duration:
            logger.info(f"Using cached matches ({len(self.cached_matches)} matches, cached {(datetime.now() - self.cached_time).total_seconds()/60:.1f} minutes ago)")
            return self.cached_matches
        
        matches = []
        match_count = 0
        
        # Fetch the main page with all games
        html_content = self._fetch_page(self.base_url)
        
        if not html_content:
            logger.error("Failed to fetch matches from MelhorOdd.pt")
            return self._get_example_matches()
        
        try:
            # Parse the HTML content
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Save the HTML to a file for debugging
            with open('melhorodd_debug.html', 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info("Saved HTML to melhorodd_debug.html for inspection")
            
            # Find all match containers - try various potential CSS selectors
            match_containers = soup.select('div.event-box')
            
            if not match_containers:
                match_containers = soup.select('div.game')
                
            if not match_containers:
                match_containers = soup.select('tr.event-row')
                
            if not match_containers:
                match_containers = soup.select('div.match-container')
                
            if not match_containers:
                # Try more selectors based on common betting site patterns
                match_containers = soup.select('div.event')
                
            if not match_containers:
                match_containers = soup.select('div.fixture')
                
            if not match_containers:
                match_containers = soup.select('div.odds-container')
                
            if not match_containers:
                # Look for any divs that might contain match information
                match_containers = soup.select('div:has(div.teams), div:has(div.match), div:has(span.team-name)')
            
            if not match_containers:
                # Last attempt with JavaScript rendered content pattern
                js_content_pattern = re.compile(r'"matches":\s*(\[.+?\])', re.DOTALL)
                js_match = js_content_pattern.search(html_content)
                
                if js_match:
                    try:
                        logger.info("Found JavaScript-rendered match data, attempting to parse")
                        matches_json = js_match.group(1)
                        matches_data = json.loads(matches_json)
                        
                        for match_data in matches_data:
                            try:
                                # Extract match info from JSON
                                match_info = {
                                    'home_team': match_data.get('homeTeam', {}).get('name', 'Unknown'),
                                    'away_team': match_data.get('awayTeam', {}).get('name', 'Unknown'),
                                    'league': match_data.get('competition', {}).get('name', 'Unknown League'),
                                    'date': datetime.fromisoformat(match_data.get('startTime', '').replace('Z', '+00:00')),
                                    'url': f"https://www.melhorodd.pt/jogo/{match_data.get('id', '')}"
                                }
                                
                                # Extract odds
                                odds = match_data.get('odds', {})
                                match_info.update({
                                    'home_win': float(odds.get('homeWin', 0)) or 0,
                                    'draw': float(odds.get('draw', 0)) or 0,
                                    'away_win': float(odds.get('awayWin', 0)) or 0,
                                    'over_2_5': float(odds.get('over25', 0)) or 0,
                                    'under_2_5': float(odds.get('under25', 0)) or 0,
                                    'btts_yes': float(odds.get('bttsYes', 0)) or 0,
                                    'btts_no': float(odds.get('bttsNo', 0)) or 0,
                                })
                                
                                matches.append(match_info)
                                match_count += 1
                            except Exception as e:
                                logger.warning(f"Error parsing JS match: {e}")
                        
                        if matches:
                            logger.info(f"Successfully extracted {match_count} matches from JavaScript data")
                            self.cached_matches = matches
                            self.cached_time = datetime.now()
                            return matches
                    except Exception as e:
                        logger.warning(f"Failed to parse JavaScript match data: {e}")
            
            if not match_containers:
                # Log all the top-level divs with classes to help debug
                top_divs = soup.select('div[class]')
                logger.info(f"Found {len(top_divs)} divs with classes. First 10 classes:")
                for i, div in enumerate(top_divs[:10]):
                    logger.info(f"Div {i+1}: class='{div.get('class')}'")
                
                # Broader search
                match_containers = soup.find_all('div', recursive=True)
                logger.warning(f"No specific match containers found, using general approach with {len(match_containers)} divs")
            
            if not match_containers:
                logger.warning("No match containers found on the page")
                return self._try_selenium_scrape()
            
            logger.info(f"Found {len(match_containers)} potential match containers")
            
            # Process each match container
            for container in match_containers:
                try:
                    # Try to extract teams
                    teams_element = container.select_one('.teams, .team-names, .event-name')
                    if not teams_element:
                        teams_element = container
                    
                    teams_text = teams_element.get_text(strip=True)
                    
                    # Skip if this doesn't look like a match
                    if 'vs' not in teams_text.lower() and '-' not in teams_text and 'â€“' not in teams_text:
                        continue
                    
                    # Extract home and away teams
                    if 'vs' in teams_text.lower():
                        home_team, away_team = teams_text.split('vs', 1)
                    elif '-' in teams_text:
                        home_team, away_team = teams_text.split('-', 1)
                    elif 'â€“' in teams_text:
                        home_team, away_team = teams_text.split('â€“', 1)
                    else:
                        # Try to find team names in separate elements
                        home_elem = container.select_one('.home-team, .team-home')
                        away_elem = container.select_one('.away-team, .team-away')
                        
                        if home_elem and away_elem:
                            home_team = home_elem.get_text(strip=True)
                            away_team = away_elem.get_text(strip=True)
                        else:
                            continue
                    
                    # Clean up team names
                    home_team = home_team.strip()
                    away_team = away_team.strip()
                    
                    # Skip if either team name is empty
                    if not home_team or not away_team:
                        continue
                    
                    # Extract match time
                    match_time_elem = container.select_one('.match-time, .time, .date, .event-time')
                    match_datetime = self._extract_date_from_match(match_time_elem)
                    
                    # Skip matches in the past
                    if not match_datetime or match_datetime < datetime.now() - timedelta(hours=3):
                        continue
                    
                    # Skip matches too far in the future
                    if match_datetime > datetime.now() + timedelta(days=self.days_ahead):
                        continue
                    
                    # Extract league/competition
                    league_elem = container.select_one('.league, .competition, .tournament')
                    league = "Unknown League"
                    if league_elem:
                        league = league_elem.get_text(strip=True)
                    
                    # Extract odds elements
                    odds_elements = container.select('.odd, .odds, .price')
                    
                    # If no odds elements found, try more selectors
                    if not odds_elements:
                        odds_elements = container.select('span[data-odd], div[data-odd]')
                    
                    if not odds_elements:
                        odds_elements = container.select('.odd-value, .odd-price')
                    
                    odds = self._parse_odds(odds_elements)
                    
                    # Create the match dictionary
                    match_info = {
                        'home_team': home_team,
                        'away_team': away_team,
                        'league': league,
                        'date': match_datetime,
                        'source': 'melhorodd',
                        'url': self.base_url,
                    }
                    
                    # Add the odds to the match info
                    match_info.update(odds)
                    
                    matches.append(match_info)
                    match_count += 1
                    
                except Exception as e:
                    logger.debug(f"Error processing match container: {e}")
                    continue
            
            logger.info(f"Successfully extracted {match_count} matches from MelhorOdd.pt")
            
            if matches:
                self.cached_matches = matches
                self.cached_time = datetime.now()
                return matches
            else:
                logger.warning("No matches found on the page")
                return self._try_selenium_scrape()
                
        except Exception as e:
            logger.error(f"Error scraping MelhorOdd.pt: {e}")
            logger.exception("Exception details:")
            return self._try_selenium_scrape()
            
    def _try_selenium_scrape(self) -> List[Dict[str, Any]]:
        """Try to scrape using Selenium as a fallback method.
        
        Returns:
            List of dictionaries containing match information
        """
        logger.info("Attempting to scrape with Selenium as fallback")
        try:
            # Import necessary libraries
            from selenium import webdriver
            from selenium.webdriver.chrome.service import Service
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from webdriver_manager.chrome import ChromeDriverManager
            
            # Configure Chrome options
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument(f'--user-agent={random.choice(self.user_agents)}')
            
            # Initialize the Chrome driver
            logger.info("Initializing Chrome WebDriver")
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            
            try:
                # Navigate to the page
                logger.info(f"Navigating to {self.base_url}")
                driver.get(self.base_url)
                
                # Wait for the page to load (wait for match containers)
                wait = WebDriverWait(driver, 20)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.event, div.match, div.fixture, table.events-table')))
                
                # Scroll down to load all content
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(3)
                
                # Get the page source after JavaScript execution
                page_source = driver.page_source
                
                # Save the page source to a file for debugging
                with open('melhorodd_selenium_debug.html', 'w', encoding='utf-8') as f:
                    f.write(page_source)
                logger.info("Saved Selenium-rendered HTML to melhorodd_selenium_debug.html")
                
                # Parse with BeautifulSoup
                soup = BeautifulSoup(page_source, 'html.parser')
                
                # Try to find match containers again
                match_containers = soup.select('div.event, div.match, div.fixture, tr.event-row')
                
                if not match_containers:
                    logger.warning("No match containers found in Selenium-rendered page")
                    return self._get_example_matches()
                
                matches = []
                match_count = 0
                
                # Process each match container (similar logic as before)
                for container in match_containers:
                    try:
                        # Extract teams
                        teams_element = container.select_one('.teams, .team-names, .event-name')
                        if not teams_element:
                            continue
                        
                        teams_text = teams_element.get_text(strip=True)
                        
                        # Skip if this doesn't look like a match
                        if 'vs' not in teams_text.lower() and '-' not in teams_text and 'â€“' not in teams_text:
                            continue
                        
                        # Extract home and away teams
                        if 'vs' in teams_text.lower():
                            home_team, away_team = teams_text.split('vs', 1)
                        elif '-' in teams_text:
                            home_team, away_team = teams_text.split('-', 1)
                        elif 'â€“' in teams_text:
                            home_team, away_team = teams_text.split('â€“', 1)
                        else:
                            continue
                        
                        # Clean up team names
                        home_team = home_team.strip()
                        away_team = away_team.strip()
                        
                        # Extract match time
                        match_time_elem = container.select_one('.match-time, .time, .date')
                        match_datetime = self._extract_date_from_match(match_time_elem)
                        
                        if not match_datetime:
                            continue
                        
                        # Extract league
                        league_elem = container.select_one('.league, .competition, .tournament')
                        league = "Unknown League"
                        if league_elem:
                            league = league_elem.get_text(strip=True)
                        
                        # Extract odds
                        odds_elements = container.select('.odd, .odds, .price')
                        odds = self._parse_odds(odds_elements)
                        
                        # Create match dictionary
                        match_info = {
                            'home_team': home_team,
                            'away_team': away_team,
                            'league': league,
                            'date': match_datetime,
                            'source': 'melhorodd_selenium',
                            'url': self.base_url,
                        }
                        
                        # Add odds
                        match_info.update(odds)
                        
                        matches.append(match_info)
                        match_count += 1
                        
                    except Exception as e:
                        logger.debug(f"Error processing Selenium match container: {e}")
                        continue
                
                logger.info(f"Successfully extracted {match_count} matches from MelhorOdd.pt with Selenium")
                
                if matches:
                    self.cached_matches = matches
                    self.cached_time = datetime.now()
                    return matches
                
            finally:
                # Close the driver
                driver.quit()
                logger.info("Closed Selenium WebDriver")
            
        except Exception as e:
            logger.error(f"Error using Selenium to scrape MelhorOdd.pt: {e}")
            logger.exception("Exception details:")
        
        # If Selenium fails, use example matches
        return self._get_example_matches()
    
    def _get_example_matches(self) -> List[Dict[str, Any]]:
        """Get example match data for testing or when scraping fails.
        
        Returns:
            List of dictionaries containing example match information
        """
        logger.warning("Using example match data since scraping failed")
        
        now = datetime.now()
        today = now.strftime("%Y-%m-%d")
        tomorrow = (now + timedelta(days=1)).strftime("%Y-%m-%d")
        
        # Create some example matches with realistic odds
        example_matches = [
            {
                "id": "melhorodd_0",
                "home_team": "Sporting CP",
                "away_team": "FC Porto",
                "league": "Primeira Liga",
                "match_time": now.replace(hour=20, minute=0, second=0).isoformat(),
                "date": today,
                "venue": "EstÃ¡dio JosÃ© Alvalade",
                "odds": {
                    "home_win": 2.15,
                    "draw": 3.25,
                    "away_win": 3.40,
                    "over_2_5": 1.95,
                    "under_2_5": 1.85,
                    "btts_yes": 1.80,
                    "btts_no": 1.90
                },
                "source": "melhorodd_example"
            },
            {
                "id": "melhorodd_1",
                "home_team": "Benfica",
                "away_team": "Braga",
                "league": "Primeira Liga",
                "match_time": now.replace(hour=21, minute=15, second=0).isoformat(),
                "date": today,
                "venue": "EstÃ¡dio da Luz",
                "odds": {
                    "home_win": 1.70,
                    "draw": 3.75,
                    "away_win": 4.50,
                    "over_2_5": 1.75,
                    "under_2_5": 2.05,
                    "btts_yes": 1.75,
                    "btts_no": 1.95
                },
                "source": "melhorodd_example"
            },
            {
                "id": "melhorodd_2",
                "home_team": "Barcelona",
                "away_team": "Real Madrid",
                "league": "La Liga",
                "match_time": (now + timedelta(days=1)).replace(hour=20, minute=0, second=0).isoformat(),
                "date": tomorrow,
                "venue": "Camp Nou",
                "odds": {
                    "home_win": 1.90,
                    "draw": 3.60,
                    "away_win": 3.80,
                    "over_2_5": 1.65,
                    "under_2_5": 2.25,
                    "btts_yes": 1.60,
                    "btts_no": 2.30
                },
                "source": "melhorodd_example"
            },
            {
                "id": "melhorodd_3",
                "home_team": "Manchester City",
                "away_team": "Liverpool",
                "league": "Premier League",
                "match_time": (now + timedelta(days=1)).replace(hour=16, minute=30, second=0).isoformat(),
                "date": tomorrow,
                "venue": "Etihad Stadium",
                "odds": {
                    "home_win": 1.75,
                    "draw": 3.80,
                    "away_win": 4.20,
                    "over_2_5": 1.60,
                    "under_2_5": 2.35,
                    "btts_yes": 1.55,
                    "btts_no": 2.40
                },
                "source": "melhorodd_example"
            }
        ]
        
        return example_matches


def scrape_melhorodd_matches(days_ahead: int = 2) -> List[Dict[str, Any]]:
    """Convenience function to scrape matches from MelhorOdd.pt.
    
    Args:
        days_ahead: Number of days ahead to scrape
        
    Returns:
        List of dictionaries containing match information
    """
    scraper = MelhorOddScraper(days_ahead=days_ahead)
    return scraper.scrape_matches()


if __name__ == "__main__":
    # Run the scraper
    matches = scrape_melhorodd_matches()
    print(f"Scraped {len(matches)} matches from MelhorOdd.pt")
    
    # Save to a JSON file for analysis
    with open("melhorodd_matches.json", "w", encoding="utf-8") as f:
        json.dump(matches, f, indent=2, ensure_ascii=False)
    
    # Print the first 3 matches
    for i, match in enumerate(matches[:3]):
        print(f"\nMatch {i+1}:")
        print(f"  {match['home_team']} vs {match['away_team']}")
        print(f"  Date: {match['date']} at {match['match_time'].split('T')[1][:5]}")
        print(f"  League: {match['league']}")
        print(f"  Odds: Home {match['odds']['home_win']} | Draw {match['odds']['draw']} | Away {match['odds']['away_win']}")
        if match['odds']['over_2_5'] > 0:
            print(f"  Over/Under 2.5: Over {match['odds']['over_2_5']} | Under {match['odds']['under_2_5']}") 